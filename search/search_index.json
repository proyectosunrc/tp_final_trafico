{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Trabajo final de la materia TR\u00c1FICO Facultad de Ingen\u00ederia - Ing. en Telecomunicaciones"},{"location":"#alumno","title":"Alumno:","text":""},{"location":"#ramiro-tizzian","title":"Ramiro Tizzian","text":""},{"location":"01-introduccion_objs/","title":"Introduccion","text":"<p>En el \u00e1mbito del cloud computing, la gesti\u00f3n eficiente de recursos, la escalabilidad din\u00e1mica y la optimizaci\u00f3n del rendimiento en entornos basados en contenedores representan desaf\u00edos cr\u00edticos que demandan soluciones te\u00f3rico-pr\u00e1cticas innovadoras. Este proyecto integra conceptos de teor\u00eda de colas, orquestaci\u00f3n de contenedores y programaci\u00f3n moderna para evaluar y desplegar un modelo de escalabilidad din\u00e1mica en servicios cloud. Combina fundamentos de teor\u00eda de colas , como el an\u00e1lisis de sistemas bajo procesos de Poisson y distribuciones exponenciales, con tecnolog\u00edas como Kubernetes, Docker y FastAPI, ofreciendo un enfoque completo para la gesti\u00f3n de tr\u00e1fico y recursos en entornos virtualizados.</p>"},{"location":"01-introduccion_objs/#objetivos","title":"Objetivos","text":""},{"location":"01-introduccion_objs/#el-trabajo-se-estructuro-en-tres-ejes-principales","title":"El trabajo se estructur\u00f3 en tres ejes principales:","text":"<p>1. Despliegue de servicios en Kubernetes: Mediante Minikube, se configur\u00f3 un cl\u00faster local que simul\u00f3 un entorno cloud realista. En este, se desplegaron servicios web y una API desarrollada con FastAPI, contenerizada con Docker, para garantizar portabilidad y consistencia.</p> <p>2. Modelado y an\u00e1lisis de redes de colas: Se estudi\u00f3 el comportamiento de los servidores bajo cargas de tr\u00e1fico modeladas con procesos estoc\u00e1sticos (arribos de Poisson y tiempos de servicio exponenciales), vinculando la  teor\u00eda de colas con m\u00e9tricas pr\u00e1cticas de rendimiento.</p> <p>3. Escalado din\u00e1mico y automatizaci\u00f3n: Se implementaron mecanismos de auto escalado horizontal en Kubernetes (HPA), optimizando la asignaci\u00f3n de capacidad seg\u00fan la demanda. Adicionalmente, se emple\u00f3 un NGINX Ingress Controller como balanceador de carga HTTP, asegurando distribuci\u00f3n eficiente del tr\u00e1fico.</p> <p>Para evaluar el sistema, se desarrollaron scripts en Python que generaron pruebas de carga y estr\u00e9s, permitiendo medir el impacto del tr\u00e1fico en la API y validar la eficacia del escalado autom\u00e1tico. La combinaci\u00f3n de contenedores ligeros (con ventajas en portabilidad y eficiencia de recursos) y herramientas de orquestaci\u00f3n como Kubernetes resulta \u00fatil  para lograr sistemas resilientes, capaces de adaptarse a fluctuaciones de demanda de tr\u00e1fico sin comprometer la estabilidad. Este proyecto no solo valida modelos te\u00f3ricos en un entorno controlado, sino que tambi\u00e9n ofrece insights pr\u00e1cticos sobre c\u00f3mo integrar tecnolog\u00edas modernas para optimizar infraestructuras en la nube. Los resultados destacan la importancia de automatizar la escalabilidad y priorizar el an\u00e1lisis de tr\u00e1fico en el dise\u00f1o de sistemas distribuidos, contribuyendo a la construcci\u00f3n de arquitecturas m\u00e1s robustas y eficientes en el ecosistema de la nube.</p>"},{"location":"02-marco_teorico/","title":"Marco te\u00f3rico","text":""},{"location":"02-marco_teorico/#conceptos-de-inicializacion","title":"Conceptos de inicializacion","text":""},{"location":"02-marco_teorico/#cluster","title":"Cluster:","text":"<p>Un cluster puede definirse como una arquitectura compuesta por m\u00faltiples nodos (servidores, m\u00e1quinas virtuales o instancias computacionales) interconectados mediante una red de alta velocidad, coordinados para operar como una sola entidad. Este dise\u00f1o busca maximizar la eficiencia del sistema a trav\u00e9s de la distribuci\u00f3n inteligente de cargas de trabajo, aprovechando la potencia combinada de sus recursos para aumentar el rendimiento, garantizar alta disponibilidad -mediante redundancia y replicaci\u00f3n de servicios- y escalar horizontalmente la capacidad de procesamiento seg\u00fan las variaciones de la demanda.</p> <p>La clave est\u00e1 en la cooperaci\u00f3n:  si una computadora del grupo tiene problemas, las dem\u00e1s autom\u00e1ticamente toman su lugar para evitar interrupciones. Adem\u00e1s, permiten agregar o quitar servidores f\u00e1cilmente, lo que ayuda a adaptarse a momentos de alta o baja actividad sin desperdiciar recursos.</p>"},{"location":"02-marco_teorico/#nodos","title":"Nodos:","text":"<p>En un cl\u00faster, las nodos est\u00e1n conectados para compartir sus recursos, como memoria, espacio de almacenamiento y capacidad de procesamiento. Esto permite repartir las tareas entre ellos de forma equilibrada, evitando que una solo se sobrecargue. La ventaja principal es que, si un nodo falla, los dem\u00e1s pueden encargarse de su trabajo al instante, manteniendo el servicio activo sin interrupciones.</p>"},{"location":"02-marco_teorico/#maquinas-virtuales","title":"Maquinas virtuales:","text":"<p>Las m\u00e1quinas virtuales funcionan mediante un software llamado hipervisor, que divide los recursos del hardware f\u00edsico, como CPU, memoria y almacenamiento, entre varias VM de manera aislada. Este aislamiento asegura que cada VM opere de forma independiente, proporcionando flexibilidad para ejecutar diferentes sistemas operativos en un solo servidor f\u00edsico.</p>"},{"location":"02-marco_teorico/#contenedor","title":"Contenedor","text":"<p>Un contenedor a\u00edsla un grupo de procesos del resto del sistema operativo, lo cual permite que cada uno tenga una vista privada de elementos como el sistema de archivos, las interfaces de red y otros recursos. Las instancias de contenedor comparten el kernel. El sistema operativo con otros procesos del sistema y puede estar restringido hasta cierto punto para usar algunos recursos inform\u00e1ticos (CPU, RAM, etc).</p>"},{"location":"02-marco_teorico/#ventajas-de-la-contenedorizacion-respecto-al-uso-de-maquinas-virtuales","title":"Ventajas de la contenedorizacion respecto al uso de maquinas virtuales","text":"<ul> <li>Facilita el despliegue rapido de aplicaciones  entre diferentes arquitecturas en la nube a diferencia de las VMs.</li> <li>Operan sobre un mismo kernel pero con aislamiento de recursos comparable a las VMs, pero con menor sobrecarga administrativa y mayor eficiencia en los tiempos de ejecuci\u00f3n. </li> <li>Eficiencia respecto a las VMs al consumir menos recursos.</li> <li>Rapido inicio al alctivarse o detenerse en cuestion de segundos, lo cual brinda mayor agilidad.</li> <li>Garantiza la portabilidad y el despliegue en cualquier entorno, sin conflictos con dependencias.</li> <li>Mas adaptables en entornos din\u00e1micos donde hay cargas de trabajo variables y la escalabilidad es bajo demanda.</li> <li>Movilidad en la nube</li> </ul>"},{"location":"02-marco_teorico/#limitaciones-frente-a-las-vms","title":"Limitaciones frente a las VMs","text":"<ul> <li>Poseen un menor aislamiento que las MVs, al estar menos aisladas no son tan seguras para aplicaciones cr\u00edticas.</li> <li>No reemplazan completamente a las VMs, ya que en escenarios dodne se requiera una aislacion total siguen siendo de utilidad    estas ultimas.  </li> </ul> <p>Figura 1: Contenedores en la Nube</p>"},{"location":"02-marco_teorico/#modelo-propuesto-en-la-publicacion-dynamic-scalability-model-for-containerized-cloud-service","title":"Modelo propuesto en la publicaci\u00f3n \"Dynamic Scalability Model for Containerized Cloud Service\"","text":"<p>El usuario final env\u00eda una solicitud de ejecuci\u00f3n de tarea a trav\u00e9s de un balanceador de carga, cuyo rol es distribuir el tr\u00e1fico de manera uniforme entre las Physical Machines (PM) disponibles en el sistema en la nube. Estas solicitudes, junto con los requisitos de ejecuci\u00f3n de otros usuarios, se almacenan temporalmente en un b\u00fafer, que act\u00faa como cola para la asignaci\u00f3n de recursos virtuales. Una vez en la cola, las tareas se dirigen secuencialmente hacia una PM disponible, siguiendo un esquema de distribuci\u00f3n equitativa. \u00a0</p> <p>Cada solicitud se asigna a un contenedor \u00fanico, garantizando aislamiento y dedicaci\u00f3n de recursos. Ante picos de demanda, el sistema implementa un escalado vertical progresivo, agregando contenedores individuales seg\u00fan se requiera, lo que permite manejar cargas variables sin sobreaprovisionamiento. \u00a0</p> <p>Para modelar el Cloud Data Center (CDC), se utiliza una red de colas abierta de Jackson (Figura 2), donde se asume que todas las PM son id\u00e9nticas y operan bajo una pol\u00edtica FIFO (First-In, First-Out).</p> <p>En este modelo:</p> <ul> <li>Las solicitudes se tratan como unidades m\u00ednimas de procesamiento (tareas), ejecutables exclusivamente en contenedores.</li> <li>Al finalizar su servicio en la PM, las tareas abandonan el sistema sin retroalimentaci\u00f3n.</li> </ul> <p>Este enfoque asegura una gesti\u00f3n eficiente de recursos, priorizando la simplicidad en la distribuci\u00f3n de cargas y la escalabilidad bajo demanda, mientras se mantiene un equilibrio entre rendimiento y consumo de recursos. La combinaci\u00f3n de balanceo de carga, contenedores din\u00e1micos y teor\u00eda de colas posiciona al sistema como una soluci\u00f3n robusta para entornos en la nube con tr\u00e1fico variable.</p> <p></p> <p>Figura 2: Modelo de colas para la nube de contenedores</p> <p>En el esquema se pueden encontrar diferentes modelos como: M/M/1, M/M/1/C y M/M/k/k, las cuales son herramientas matem\u00e1ticas empleadas en la teor\u00eda de colas para analizar el rendimiento y comportamiento de sistemas de espera, utilizando la notaci\u00f3n de Kendall. Cada componente de esta notaci\u00f3n describe una caracter\u00edstica espec\u00edfica del sistema. Por ejemplo, un modelo M/M/k/k se interpreta de la siguiente manera:</p> <ul> <li>M: Indica que las llegadas al sistema siguen un proceso de Poisson, lo que implica que ocurren de forma aleatoria e independiente  en el tiempo, con una tasa de llegada denotada por \u03bb (lambda).</li> <li>M: Representa que los tiempos de servicio tambi\u00e9n siguen un proceso de Poisson, con una tasa de servicio indicada por \u03bc (mu).</li> <li>k: Corresponde al n\u00famero total de servidores disponibles en el sistema.</li> <li>k: Define la capacidad m\u00e1xima del sistema, es decir, la cantidad m\u00e1xima de solicitudes que pueden estar presentes simult\u00e1neamente.</li> </ul> <p>A continuaci\u00f3n, se muestran las gr\u00e1ficas del paper. Analizaremos lo que se puede observar en cada una de ellas.</p>"},{"location":"03-desarrollo/","title":"Desarrollo del proyecto","text":""},{"location":"03-desarrollo/#herramientas-empleadas","title":"Herramientas empleadas","text":""},{"location":"03-desarrollo/#load-balancer-balanceador-de-cargas","title":"Load Balancer (balanceador de cargas)","text":"<p>El concepto de balanceador de cargas (Load Balancer) utilizado en la administraci\u00f3n de sistemas inform\u00e1ticos que consiste en distribuir un conjunto de peticiones o tareas entre diversos recursos, como computadoras, procesadores u otros dispositivos, para equilibrar la carga de trabajo. Esto se logra mediante distintos algoritmos que determinan la forma de reparto de las solicitudes, y en este caso se ha optado por el algoritmo Round Robin.</p> <p>El m\u00e9todo Round Robin es un algoritmo de planificaci\u00f3n simple y justo, empleado tanto en sistemas operativos como en diversas aplicaciones, para gestionar la ejecuci\u00f3n de procesos o tareas. Su funcionamiento se basa en asignar a cada recurso un intervalo de tiempo, denominado quantum, de manera c\u00edclica. Por ejemplo, si el quantum es de 10 milisegundos, y consideramos tres tareas identificadas como A, B y C, la tarea A se ejecutar\u00e1 durante los primeros 10 ms, luego la tarea B, despu\u00e9s la tarea C y, una vez concluido el ciclo, se vuelve a comenzar con la tarea A. Si alguna tarea no se completa durante su quantum, se reubica al final de la cola para recibir otro segmento de tiempo en futuras iteraciones. Este enfoque garantiza que todas las tareas obtengan una parte equitativa de los recursos del sistema, evitando que alguna monopolice la CPU de forma indefinida.</p>"},{"location":"03-desarrollo/#ingress","title":"Ingress","text":"<p>El Ingress en un cl\u00faster de Kubernetes es un recurso que administra el acceso externo a los servicios internos, especialmente para tr\u00e1fico HTTP y HTTPS. Su funcionamiento se puede resumir en los siguientes puntos:</p> <ul> <li>Define reglas: basadas en dominios (hosts) y rutas (paths) para dirigir el tr\u00e1fico a servicios espec\u00edficos dentro del cl\u00faster.</li> <li>Controlador de Ingress: Para que tenga efecto el recurso ingress, se requieren de controladores como el de NGINX, para interpretar y aplicar las reglas definidas en el recurso ingress.</li> <li>Balanceo de carga y seguridad: El controlador act\u00faa como un balanceador de carga y puede manejar aspectos de seguridad como certificados TLS, redireccionamientos y reescrituras de URLs.</li> <li>Flexibilidad en la configuraci\u00f3n: Permite centralizar la gesti\u00f3n del tr\u00e1fico entrante sin necesidad de exponer directamente cada servicio, facilitando la administraci\u00f3n y la escalabilidad.</li> <li>Rendimiento: El rendimiento general depender\u00e1 del controlador de Ingress utilizado, en este caso se utilizar\u00e1 NGINX permitiendo a\u00f1adir capas de autenticaci\u00f3n, restricciones por IP, o rate-limiting para proteger las APIs.</li> </ul>"},{"location":"03-desarrollo/#nginx","title":"NGINX","text":"<p>NGINX es uno de los servidores web y balanceadores de carga m\u00e1s populares y vers\u00e1tiles, utilizado tanto para servir contenido web como para distribuir de manera eficiente el tr\u00e1fico entrante entre m\u00faltiples servidores o recursos. Su funcionamiento se basa en los siguientes puntos clave:</p> <ul> <li>Recepcion y reedireccion de solicitudes</li> <li>Monitoreo y alta disponibilidad</li> <li>Optimizacion y eficiencia</li> <li>Soporte y compatibilidad</li> <li>Facilidad de configuraci\u00f3n y extensibilidad</li> </ul> <p>NGINX funciona como balanceador de cargas al actuar como intermediario entre los clientes y un conjunto de servidores backend. Su operaci\u00f3n se puede describir en los siguientes pasos:</p> <ol> <li> <p>Recepci\u00f3n de solicitudes:  NGINX escucha las solicitudes entrantes en un puerto espec\u00edfico (por ejemplo, el 80 para HTTP o el 443 para HTTPS).</p> </li> <li> <p>Distribuci\u00f3n de la carga:  Al recibir una solicitud, NGINX utiliza una estrategia de balanceo de carga para seleccionar el servidor backend al que se dirigir\u00e1 la petici\u00f3n. Politicas de distribucion de la carga:</p> <ul> <li>Round Robin: Las solicitudes se distribuyen de manera c\u00edclica y equitativa entre los servidores.</li> <li>Least Connections: La solicitud se dirige al servidor con el menor n\u00famero de conexiones activas.</li> <li>IP Hash: Se utiliza la direcci\u00f3n IP del cliente para determinar el servidor, lo que puede ayudar a mantener la persistencia de sesi\u00f3n.</li> </ul> </li> <li> <p>Chequeo de salud: Monitoreo continuo del  estado de los servidores backend. Si detecta que alguno no est\u00e1 disponible o presenta fallos, evitar\u00e1 enviarle nuevas solicitudes hasta que vuelva a estar operativo, garantizando as\u00ed la alta disponibilidad del servicio.</p> </li> <li> <p>Procesamiento y respuesta: Una vez que el servidor backend seleccionado procesa la solicitud, NGINX recibe la respuesta y la env\u00eda de vuelta al cliente. Este proceso permite distribuir la carga de trabajo de forma eficiente y optimizar el tiempo de respuesta.</p> </li> <li> <p>Optimizaci\u00f3n y seguridad: Adem\u00e1s del balanceo de cargas, NGINX puede ofrecer funcionalidades adicionales como la terminaci\u00f3n de conexiones seguras mediante SSL/TLS, cach\u00e9, y compresi\u00f3n, lo que contribuye a mejorar la eficiencia y seguridad en la gesti\u00f3n del tr\u00e1fico.</p> </li> </ol> <p></p> <p>Figura 3: NGINX como balanceador de carga</p>"},{"location":"03-desarrollo/#kubernetes","title":"Kubernetes","text":"<p>Kubernetes es una plataforma de orquestaci\u00f3n de contenedores de c\u00f3digo abierto dise\u00f1ada para automatizar el despliegue, la gesti\u00f3n y el escalado de aplicaciones en contenedores.</p> <p>Para este proyecto se utilizo el entorno de Minikube en donde se permite desplegar un cluster de kubernetes dentro de un entorno local, ideal para el desarrollo y las pruebas.Como ventaja, proporciona una forma sencilla de experimentar con k8s sin la necesidad de desplegar una infraestructura completa, ademas de consumir menos recursos.</p> <p>Para ello se instala el entorno minikube configurando un cluster ligero en nuestra PC, funcionando tanto en maquinas virtuales como (virtual Box, kvm, VMware, etc) o en contenedores como Docker.</p>"},{"location":"03-desarrollo/#requisitos-de-hardware-minimos-para-usar-minikube","title":"Requisitos de hardware minimos para usar Minikube","text":"<p>Sistema Operativo Compatible: Minikube es compatible con Linux, macOS y Windows.</p> <p>Memoria RAM: Se recomiendan al menos 2 GB de RAM para ejecutar el cl\u00faster, aunque para un rendimiento m\u00e1s fluido es aconsejable contar con 4 GB o m\u00e1s.</p> <p>Procesador (CPU): Se sugiere disponer de al menos 2 n\u00facleos de CPU.</p> <p>Espacio en Disco: Alrededor de 20 GB de espacio libre en disco para la m\u00e1quina virtual o contenedor en el que se ejecutar\u00e1 Minikube.</p> <p>Entorno de Virtualizaci\u00f3n o Contenedores: Es necesario contar con un hipervisor (como VirtualBox, VMware o Hyper-V) o utilizar el controlador Docker para gestionar la m\u00e1quina virtual o el entorno de contenedores en el que se levantar\u00e1 el cl\u00faster.</p> <p>kubectl: Tener instalado el cliente de Kubernetes -kubectl- para interactuar y administrar el cl\u00faster desplegado.</p>"},{"location":"03-desarrollo/#addons-de-minikube","title":"Addons de Minikube","text":"<p>Son componentes adicionales que posee el cluster y que permiten configuraciones predefinidas que se habilitan para extender algunas funcionalidades. Los que se utilizaron para este proyecto son los siguientes:</p> <ul> <li>Metrics Server: Recopila m\u00e9tricas de recursos del cl\u00faster y las hace accesibles para consultas.</li> <li>Ingress: Facilita la exposici\u00f3n de servicios HTTP y HTTPS desde el cl\u00faster a trav\u00e9s de reglas de enrutamiento.</li> <li>Dashboard: Proporciona una interfaz web que permite visualizar y administrar recursos en tu cl\u00faster de Kubernetes.</li> </ul>"},{"location":"03-desarrollo/#kubectl","title":"Kubectl","text":"<p>Es la herramienta de linea de comandos para interactuar y administrar un cluster en Kubernetes.</p> <p>Su utilidad radica en que permite el despliegue y gestion de recursos: Permitiendo crear, actualizar y elimnar objetos como pods, servicios, deploymentos y otros componentes en archivos de configuracion YAML o por medio de comandos.</p> <p>Permite la interacci\u00f3n con la API de Kubernetes: comunicandose derectamente con el servidor API del cluster para enviar comandos y recuperar informacion sobre el estado de recursos.</p> <p>Depuracion y monitoreo: Facilitar la visualizaci\u00f3n de logs, inspecci\u00f3n del estado de los contenedores y solucion a problemas que pueda tener el cluster Minikube.</p> <p>Automatizaci\u00f3n de tareas: Tambien ayuda a la automatizacion de flujos de trabajo para el desarrollo y pruebas, haciendo que la administracion del cluster sea mas eficiente.</p> <p></p>"},{"location":"04-implementacion/","title":"Implementacion del escenario","text":""},{"location":"04-implementacion/#diagrama-de-red","title":"Diagrama de Red","text":"<p>EL diagrama se puede encontrar en el siguiente enlace:</p> <p>Diagrama</p>"},{"location":"04-implementacion/#uso-del-framework-web-fastapi-para-la-creacion-del-servidor","title":"Uso del Framework WEB FastAPI para la creaci\u00f3n del servidor","text":"<p>Se detalla como alojar el servidor que recibira las peticiones desde el cliente, con el se crearan los endpoints para devolver los tiempos de respuesta de cada peticion siguiendo una distribucion exponencial- </p> <p>Para mas detalles del codigo: script del servidor</p> <p><pre><code>.\n\u251c\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 server.py\n</code></pre> requirements.txt <pre><code>fastapi\nuvicorn\n</code></pre></p> <p>Dockerfile</p> <pre><code># Usamos una imagen base oficial de Python\nFROM python:3.11-slim\n\n# Establecemos el directorio de trabajo en la imagen\nWORKDIR /app\n\n# Copiamos el archivo requirements.txt (si tienes uno) a la imagen\nCOPY requirements.txt /app/\n\n# Instalamos las dependencias de Python\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copiamos el c\u00f3digo de la aplicaci\u00f3n a la imagen\nCOPY . /app/\n\n# Exponemos el puerto que usa la aplicaci\u00f3n FastAPI (por defecto es 8000)\nEXPOSE 8000\n\n# Comando para ejecutar la aplicaci\u00f3n usando Uvicorn\nCMD [\"uvicorn\", \"server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>"},{"location":"04-implementacion/#dockerizacion-del-servidor-para-despliegue-en-kubernetes","title":"Dockerizacion del servidor para despliegue en Kubernetes","text":"<pre><code>#Nos ubicamos dentro de la carpeta donde estan los tres archivos antes mencionados\ndocker build -t \"nombre de usuario\"/miapp:latest .\ndocker push \"nombre de usuario\"/miapp:latest\n</code></pre>"},{"location":"04-implementacion/#despligue-del-cluster-con-minikube","title":"Despligue del cluster con Minikube","text":""},{"location":"04-implementacion/#instalacion-de-minikube","title":"Instalacion de Minikube","text":"<pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube\n</code></pre>"},{"location":"04-implementacion/#inicializacion-del-cluster","title":"Inicializaci\u00f3n del cluster","text":"<pre><code>minikube start --driver=\"virtualbox\" --memory=\"ram\" --cpus=\"cantidad\" --nodes=\"cantidad de nodos\"\n</code></pre>"},{"location":"04-implementacion/#se-activan-algunos-addons","title":"Se activan algunos addons:","text":"<pre><code>minikube addons enable ingress\nminikube addons enable metrics-server\nminikube addons enable dashboard\n#Inicializa el dashboard\n\nminikube dashboard\n</code></pre> <p>Una vez iniciado el cluster se procede a deployar los archivos de configuraci\u00f3n .yaml:</p>"},{"location":"04-implementacion/#kubectl","title":"Kubectl","text":""},{"location":"04-implementacion/#instalacion","title":"Instalacion","text":"<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\n#Validar binario\ncurl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256\"\necho \"$(cat kubectl.sha256)  kubectl\" | sha256sum --check\n</code></pre>"},{"location":"04-implementacion/#validar-version","title":"Validar version","text":"<pre><code>kubectl version --client\n</code></pre> <pre><code>kubectl apply -f (archivo.yaml)\n</code></pre>"},{"location":"04-implementacion/#visualiza-servicios","title":"Visualiza servicios:","text":"<pre><code>kubectl get svc\n</code></pre>"},{"location":"04-implementacion/#visualiza-pods","title":"Visualiza PODs:","text":"<pre><code>kubectl get pods\n</code></pre>"},{"location":"04-implementacion/#muestra-el-consumo-de-cpu-de-los-pods","title":"Muestra el consumo de CPU de los PODs:","text":"<pre><code>kubectl top pods\n</code></pre>"},{"location":"04-implementacion/#visualizar-todo-el-cluster","title":"Visualizar todo el cluster:","text":"<pre><code>kubectl get all\n</code></pre>"},{"location":"04-implementacion/#aplicacion-de-los-archivos-de-configuracion","title":"Aplicacion de los archivos de configuracion","text":"<pre><code>kubectl apply -f 01-deployment.yaml 02-service.yaml 03-ingress.yaml\n</code></pre>"},{"location":"04-implementacion/#tipos-de-archivos-yaml","title":"Tipos de archivos YAML:","text":"<p>Los implementados en este proyecto:</p> <ul> <li> <p>Deployments: Un \"Deployment\" en Kubernetes es un objeto que permite declarar y gestionar la implementaci\u00f3n de aplicaciones en un cl\u00faster. Proporciona funcionalidades como estrategias de despliegue, escalabilidad y rollbacks, facilitando la gesti\u00f3n del ciclo de vida de las aplicaciones en entornos contenerizados.</p> </li> <li> <p>Service: Un \"Service\" en Minikube es como un operador de tr\u00e1fico para tus aplicaciones en Kubernetes. Le da a tus aplicaciones una direcci\u00f3n fija y f\u00e1cil de recordar para que puedan comunicarse entre s\u00ed sin importar d\u00f3nde est\u00e9n ejecut\u00e1ndose. Tambi\u00e9n puede repartir el tr\u00e1fico entre varias partes de tu aplicaci\u00f3n para mantener las cosas equilibradas. En resumen, hace que sea m\u00e1s f\u00e1cil para las partes de tu aplicaci\u00f3n encontrarse y hablar entre s\u00ed.</p> </li> <li> <p>Ingress: es un recurso de Kubernetes que proporciona una forma de gestionar y controlar el acceso externo a los servicios que est\u00e1n dentro de un cl\u00faster de Kubernetes. A trav\u00e9s de Ingress, se definen reglas que permiten redirigir el tr\u00e1fico de entrada (por ejemplo, HTTP o HTTPS) a los servicios internos del cl\u00faster bas\u00e1ndose en criterios como la URL, el host, o el puerto.</p> </li> </ul> <p>Enlace a Scripts</p>"},{"location":"04-implementacion/#uso-del-comando-socat","title":"Uso del comando socat","text":"<p>Se utilizo la herramienta \"socat\" para que la PC anfitriona f\u00edsica pueda redirigir el tr\u00e1fico de solicitudes al puerto despecificado como NodePort en el ingress del cluster. Esto se debe hacer ya que el cluster est\u00e1 aislado dentro de la PC anfitriona y nadie desde fuera puede llegar de forma directa. El comando que se utiliza es el siguiente:</p> <pre><code>#Para ver el puerto del ingress\nkubectl get svc -A\n\nsocat tcp-listen:\"puerto PC Host\",bind=\"IP del Host\",reuseaddr,fork tcp4:\"Minikube IP\":\"Puerto ingress\"\n</code></pre>"},{"location":"04-implementacion/#configuracion-en-el-cliente","title":"Configuracion en el cliente:","text":"<p><pre><code>sudo nano /etc/hosts\n</code></pre> Luego, en ese archivo se agrega la IP del servidor y el nombre de la app:</p> <pre><code>\"IP host\" miapp.local\n</code></pre>"},{"location":"04-implementacion/#generacion-de-trafico","title":"Generacion de trafico","text":"<p>Para la generacion de trafico por parte de los clientes se empleo un codigo que simula la carga enviando solicitudes HTTP as\u00edncronas a la URL del servidor donde se aloja el cluster y el puerto del mismo, durante un periodo de tiempo determinado. Las solicitudes se generan basadas en una distribucion de tiempos exponencial, simulando el trafico web con tiempos de arribo aleatorios.</p> <p>Mas informacion</p>"},{"location":"04-implementacion/#balanceador-de-cargas","title":"Balanceador de cargas","text":"<p>Para gestionar todas las solicitudes entrantes y aprovechar la escalabilidad de la nube, modelamos el balanceador de carga como un sistema de colas M/M/1. En este modelo, la cola tiene capacidad infinita y las solicitudes de tareas llegan de manera individual. Las llegadas de tareas siguen un proceso de Poisson con una tasa \u03bb, lo que significa que los tiempos entre llegadas son independientes y siguen una distribuci\u00f3n exponencial con una media de 1/\u03bb. Por otro lado, los tiempos de servicio en el servidor del balanceador de carga tambi\u00e9n siguen una distribuci\u00f3n exponencial, con un par\u00e1metro de velocidad \u03bc, donde 1/\u03bc representa el tiempo medio de servicio. En este contexto:</p> <ul> <li> <p>La primera \"M\" representa el proceso de arribo de solicitudes, el cual puede ser controlado mediante el codigo en Python del generador de tr\u00e1fico, ajustando el tiempo entre llegadas.</p> </li> <li> <p>La segunda \"M\" corresponde al proceso de servicio, que tambi\u00e9n sigue una distribuci\u00f3n exponencial, determinada por el tiempo de atenci\u00f3n de cada solicitud en el script de la aplicaci\u00f3n del servidor en python.</p> </li> </ul> <p>Para esta implementaci\u00f3n, se emplea una \u00fanica m\u00e1quina f\u00edsica (PM) y se asume que la cola en NGINX tiene capacidad infinita, lo que permite analizar el comportamiento del sistema sin restricciones de perdidas en la cola.</p>"},{"location":"04-implementacion/#nodos","title":"Nodos","text":"<p>En esta etapa, el proceso de arribo tambi\u00e9n se modela con \"M\", donde la nueva tasa de llegada se calcula dividiendo la tasa de arribo total entre la cantidad de nodos trabajadores (worker nodes). En cuanto al proceso de servicio, se presenta una situaci\u00f3n similar a la de NGINX, donde es complicado determinar la distribuci\u00f3n exacta que rige este proceso. Esto se debe a que el servicio puede depender de factores que no son f\u00e1cilmente controlables, como la configuraci\u00f3n y el comportamiento del sistema en los nodos trabajadores.</p> <p>Al definir la capacidad del sistema \"C\", se aclara que Kubernetes no gestiona colas de tareas, por lo que inicialmente C=1. Aunque esto podr\u00eda llevar a p\u00e9rdidas en cada nodo, en la pr\u00e1ctica no se observan p\u00e9rdidas debido al protocolo TCP sobre HTTP, que garantiza la transmisi\u00f3n de tareas a costa de mayor latencia. Esto se logra mediante herramientas como el control de ventana y la retransmisi\u00f3n, lo que puede generar colas tanto en el cliente como en el servidor.</p>"},{"location":"04-implementacion/#pods-servidores","title":"PODs / Servidores","text":"<p>Los pods dentro de un nodo se pueden representar por un modelo de colas M/M/k/k , la primera \"M\" en notacion de Kendall representa la tasa de arrivo divida por la cantidad de contenedores. </p> <p>La segunda \"M\" representa el tiempo de procesamiento del pod definido en la app del servidor FastAPI siguiendo una distribucion exponencial, lo cual es modificable. Con relacion a la cantidad de servidores, se tienen \"k\" de los mismos representado por la cantidad de pods  que pueden atender solicitudes en simultaneo, en este caso son los encargados de procesar las solicitudes de tareas.</p> <p>Por otro lado, se considero a cada pod como un \u00fanico servidor, por lo que cada uno puede resolver una solicitud a la vez, las nuevas solicitudes deben esperar en cola hasta que el pod este disponible. Por eso la capacidad maxima de la cola tambien se ver\u00e1 limitada por la cantidad de servidores \"k\" (pods), entonces si todos los servidores estan procesando solicitudes y se alcanza el limite, las nuevas solicitudes se rechazan o se enrutan hacia otro nodo.</p>"},{"location":"05-resultados/","title":"Resultados","text":""},{"location":"05-resultados/#consideraciones","title":"Consideraciones","text":"<p>Para realizar las pruebas de carga se tuvieron en cuenta las siguientes consideraciones:</p> <ol> <li>La tasa de servicio del servidor es \ud835\udf07=10 como parametro de la distribucion exponencial. Definido en el archivo server.py.</li> <li>La tasa de arrivo \ud835\udf06 se varia aumentando la carga en cada una de las pruebas.</li> <li>La utilizaci\u00f3n del sistema \ud835\udf0c= \ud835\udf06/\ud835\udf07 ser\u00e1 el valor que se desea obtener en las pruebas.</li> <li>El tiempo de respuesta por solicitud T=1/(\ud835\udf07-\ud835\udf06), el cual se va a utilizar para comprar los valores teoricos y los obtenidos en la practica, para mas de un pod se calcula como T=1/(\ud835\udf07*k-\ud835\udf06).</li> </ol>"},{"location":"05-resultados/#tabla-i-resultados-de-las-mediciones","title":"Tabla I: Resultados de las mediciones","text":"<p>En la Tabla 1 se presentan los distintos escenarios dise\u00f1ados para analizar el comportamiento del sistema. El ensayo consisti\u00f3 en realizar 28 pruebas distintas, en las cuales se modificaron diferentes par\u00e1metros con el objetivo de evaluar los resultados obtenidos en cada caso. Durante las pruebas, se vari\u00f3 la cantidad de pods en el cl\u00faster y, para cada configuraci\u00f3n, se ajustaron los valores te\u00f3ricos de \u03bb (tasa de llegada de solicitudes) en funci\u00f3n de la carga del sistema \u03c1 que se deseaba analizar.</p> <p>En la pr\u00e1ctica, se observ\u00f3 el comportamiento real de los pods y se calcul\u00f3 el promedio de utilizaci\u00f3n de milicores en el cl\u00faster. Con este valor, se obtuvo el \u03c1 pr\u00e1ctico, lo que permiti\u00f3 determinar el \u03bc pr\u00e1ctico bas\u00e1ndose en la ecuaci\u00f3n del sistema para un modelo M/M/C, donde el n\u00famero de servidores C corresponde a la cantidad de pods configurados en el cl\u00faster.</p>"},{"location":"05-resultados/#ecuaciones-utilizadas-para-modelar-el-sistema","title":"Ecuaciones utilizadas para modelar el sistema","text":"<p>Donde k son la cantidad de PODS.</p>"},{"location":"05-resultados/#tiempos-de-respuesta-del-sistema-en-funcion-de-la-tasa-de-arribo","title":"Tiempos de Respuesta del sistema en funcion de la tasa de arribo:","text":"<p>Para valores bajo de carga del sistema \u03c1, el tiempo de respuesta es peque\u00f1o y estable, pero cuando se acerca a 1 o lo que es lo mismo cuando  \u03bb se acerca demasiado a \u03bc, el tiempo de respuesta crece r\u00e1pidamente y el pod ya no puede manejar la carga de trabajo de manera efectiva lo que indica congestion y demoras altas.</p> <p></p>"},{"location":"05-resultados/#numero-de-tareas-en-el-sistema-en-funcion-de-la-tasa-de-arribo","title":"Numero de tareas en el sistema en funcion de la tasa de arribo:","text":"<p>Para valores bajos de \u03bb, el numero esperado de tareas en el sistema crece de manera controlado, pero a medida que la tasa de arrivos se acerca a la capacidad del sistema (k\u03bc), L se dispara, indicando sobrecarga y congestion. Cuando \u03bb supera la capacidad del sistema, L tiende a infinito, y el sistema es inestable</p> <p></p>"},{"location":"05-resultados/#consumo-de-cpu-en-el-cluster-en-funcion-de-la-tasa-de-arribo","title":"Consumo de CPU en el cluster en funcion de la tasa de arribo:","text":"<p>El consumo de CPU en el cluster para los valores de lambda tomados crece en porcentaje a medida que se incrementa la tasa de arrivos, la utilidad en el sistema aumenta por lo tanto es mayor el consumo de recursos de hardware.</p> <p></p>"},{"location":"05-resultados/#uso-de-cpu-en-la-maquina-anfitriona-con-el-cluster-corriendo-en-reposo-y-realizando-una-prueba-de-carga","title":"Uso de CPU en la maquina anfitriona con el cluster corriendo en reposo y realizando una prueba de carga:","text":""},{"location":"05-resultados/#tasa-de-perdida-en-funcion-de-la-tasa-de-arribo","title":"Tasa de perdida en funcion de la tasa de arribo:","text":"<p>Al aumentar la tasa de arribo \u03bb, el sistema tiende a saturarse, lo que provoca un aumento en el tiempo de respuesta, una mayor tasa de p\u00e9rdida de solicitudes y una alta utilizaci\u00f3n de recursos.</p> <p></p> <p> </p>"},{"location":"05-resultados/#respuesta-del-sistema-en-funcion-de-la-cantidad-de-servidores-en-ejecucion-para-un-lambda15-1s-y-mu10-1s","title":"Respuesta del sistema en funcion de la cantidad de servidores en ejecucion para un lambda=15 1/s y mu=10 1/s","text":"<p>Scripts para graficar.</p> <p> </p>"},{"location":"05-resultados/#tiempo-de-respuesta-promedio-en-funcion-del-factor-de-utilidad-para-los-valores-medidos-variando-la-cantidad-de-pods","title":"Tiempo de respuesta promedio en funcion del factor de utilidad para los valores medidos variando la cantidad de pods:","text":""},{"location":"05-resultados/#uso-del-horizontal-pod-autoescaler","title":"Uso del Horizontal Pod Autoescaler","text":""},{"location":"05-resultados/#caso-1-1-pod-30","title":"Caso 1: 1 Pod, \u03bb=30","text":"<p>Configurando el HPA para que escale hasta un maximo de 6 pods cuando el uso promedio del CPU exceda el 25%, para este caso con una tasa de arribo igual a 30 1/s y para un pod en ejecucion el sistema se estabiliza aproximadamente a las 7000 solicitudes alcanzando el escalado maximo de 6 replicas.</p>"},{"location":"05-resultados/#caso-2-2-pod-30","title":"Caso 2: 2 Pod, \u03bb=30","text":""},{"location":"05-resultados/#caso-3-2-pod-40","title":"Caso 3: 2 Pod, \u03bb=40","text":"<p>Horizontal Pod Autoescaler</p>"},{"location":"06-conclusiones/","title":"Conclusiones","text":"<p>Se logr\u00f3 desarrollar e implementar un cl\u00faster local en Kubernetes, analizando su comportamiento y escalabilidad bajo distintas condiciones de tr\u00e1fico. A partir de los conceptos te\u00f3ricos de redes de colas y tomando como referencia el paper base, se interpret\u00f3 el funcionamiento de los principales componentes del proyecto, como el balanceador de carga, los nodos y los contenedores, dentro del entorno de Kubernetes.</p> <p>Si bien el modelo implementado no replica exactamente el del paper de referencia, en la pr\u00e1ctica el encolamiento se llev\u00f3 a cabo en el servidor Uvicorn dentro de cada pod, funcionando individualmente como un modelo M/M/1. Mientras tanto, el Ingress y los nodos se limitaron a redirigir las solicitudes a los pods siguiendo la pol\u00edtica de distribuci\u00f3n Round-Robin establecida.</p> <p>Los resultados obtenidos muestran que el comportamiento del sistema sigue las ecuaciones te\u00f3ricas del tr\u00e1fico en colas, alcanzando valores similares a los esperados en las pruebas realizadas. En cuanto al desempe\u00f1o del cl\u00faster bajo pruebas de carga, se observ\u00f3 un aumento en la carga del sistema al incrementar la tasa de arribo con una cantidad fija de servidores. Asimismo, los tiempos de respuesta se mantuvieron estables mientras la utilizaci\u00f3n del CPU en el cl\u00faster fue inferior al 100%. Sin embargo, al superar este umbral, la latencia se increment\u00f3 significativamente y el sistema se volvi\u00f3 inestable. Por otro lado, se verific\u00f3 que aumentar la cantidad de pods permiti\u00f3 estabilizar los tiempos de respuesta para una misma tasa de arribo.</p> <p>A trav\u00e9s de simulaciones pr\u00e1cticas y modelado matem\u00e1tico, se comprob\u00f3 que el sistema puede adaptarse eficientemente a variaciones en la carga mediante el uso de un Horizontal Pod Autoscaler (HPA). Esta estrategia permiti\u00f3 una respuesta din\u00e1mica ante fluctuaciones en la demanda, optimizando el uso de los recursos y asegurando una calidad de servicio adecuada.</p>"},{"location":"07-networking/","title":"Networking","text":"<p>En la PC anfitrion donde se aloja el cluster, se tienen varios niveles que interactuan para dirigir el trafico interno y externo del cluster. La red interna y el POD Networking, donde cada pod creado se le configura una IP privada definida por el plugin de red (por jemplo, 10.244.0.0/16). A estas IPs se puede acceder solamente desde dentro del cluster. Los servicios de tipo ClusterIP generan una IP  virtual interna que permite que los pods se comuniquen entre s\u00ed y se distribuya la carga. Este Service no es accesible directamente desde el exterior.</p> <p>En cuanto al recurso Ingress con el controlador NGINX, este se despliega en uno o varios pods y expone por los puertos 80 y 443 una entrada para el tr\u00e1fico externo. Por medio de reglas basadas en host y path, el ingress redirige el tr\u00e1fico a los Services correspondientes dentro del cluster.</p> <p>Mientras que Minikube generalmente se ejecuta en una m\u00e1quina virtual ( o en un contenedor, dependiendo del driver usado), tiene su propia red interna la cual conecta el cluster (nodos, pods, ingress, etc). Pero no se puede acceder directamente de manera externa a la red del cluster.</p> <p>Para poder acceder externamente, se utilizo la  herramienta socat ejecutandose en la PC anfitriona para exponer el cluster al exterior. Lo que socat hace es redirigir el tr\u00e1fico que llega externamente al puerto de la PC anfitrion, y de all\u00ed lo env\u00eda al puerto de entrada del cluster que corresponde al del ingress en este caso. Permitiendo de este modo, por mas de que PODs y servicios tengan IPs internas, canalizar el trafico externo hacia el cluster y poder llegar a la API corriendo en los pods.</p> <p> </p> <p>Solicitud:</p> <p>Se observan las IPs de origen y destino asi como tambien los puertos que forman parte entre el host cliente y la interfaz de red de la PC anfitrion. Luego, el socat se encarga de realizar el port forwarding mapeando el puerto 31042 de la PC host con el del ingress para tener acceso al cluster.</p> <p></p> <p> </p> <p>Respuesta:</p> <p>Despu\u00e9s de que los pods procesan la solicitud y generan una respuesta, esta es enviada de regreso al Ingress, que se encarga de transferirla desde la PC anfitrion hacia el cliente nuevamente con la informaci\u00f3n para almacenar en un archivo JSON.</p> <p></p>"},{"location":"coding/implementacion_bash/","title":"bash","text":""},{"location":"coding/implementacion_bash/#calcula-el-factor-de-utilidad-en-funcion-del-consumo-de-cpu-de-los-pods-dentro-del-cluster-cuando-se-realizan-pruebas-de-carga","title":"Calcula el factor de utilidad en funcion del consumo de CPU de los PODs dentro del cluster cuando se realizan pruebas de carga:","text":"<pre><code>#!/bin/bash\n\n# Intervalo de recolecci\u00f3n en segundos y n\u00famero m\u00e1ximo de muestras\ninterval=5\nmax_samples=100\n\nsuma_total=0\ncontador=0\n\n# Capacidad total del cl\u00faster en milicores (1950m + 2020m)\ncpu_total_cluster=1950\n\n# Funci\u00f3n para calcular y mostrar el promedio y el factor de utilizaci\u00f3n\ncalcular_promedio() {\n    if [ $contador -eq 0 ]; then\n        echo \"No se ha recolectado ninguna muestra.\"\n    else\n        promedio=$(echo \"scale=2; $suma_total / $contador\" | bc)\n        factor_utilizacion=$(echo \"scale=4; $promedio / $cpu_total_cluster\" | bc)\n        echo \"Promedio de la suma de milicores de los nodos: ${promedio}m\"\n        echo \"Factor de utilizaci\u00f3n promedio del cl\u00faster: ${factor_utilizacion} (o $(echo \"scale=2; $factor_utilizacion * 100\" | bc)% de uso)\"\n    fi\n}\n\n# Capturamos la se\u00f1al de interrupci\u00f3n (Ctrl+C)\ntrap 'echo; echo \"Ejecuci\u00f3n interrumpida.\"; calcular_promedio; exit 0' SIGINT\n\necho \"Iniciando recolecci\u00f3n de m\u00e9tricas de CPU (milicores) en dos nodos...\"\necho \"Presiona Ctrl+C para interrumpir la ejecuci\u00f3n y ver el promedio y el factor de utilizaci\u00f3n.\"\n\nwhile [ $contador -lt $max_samples ]; do\n    # Se extrae y suma la utilizaci\u00f3n en milicores de los nodos\n    suma_muestra=$(kubectl top nodes --no-headers | awk '{gsub(\"m\",\"\",$2); sum+=$2} END {print sum}')\n\n    contador=$((contador + 1))\n    echo \"Muestra $contador: Suma de utilizaci\u00f3n = ${suma_muestra}m\"\n\n    suma_total=$(echo \"$suma_total + $suma_muestra\" | bc)\n    sleep $interval\ndone\n\ncalcular_promedio\n</code></pre>"},{"location":"coding/implementacion_py/","title":"python","text":""},{"location":"coding/implementacion_py/#servidor-de-respuestas-a-peticiones-http","title":"Servidor de respuestas a peticiones HTTP","text":""},{"location":"coding/implementacion_py/#servidor","title":"Servidor","text":"<pre><code>from fastapi import FastAPI\nimport random\nimport time\nimport asyncio\n\n# Inicializar la aplicaci\u00f3n FastAPI\napp = FastAPI()\n\n# Definir la tasa media de servicio\nSERVICE_RATE = 10  \n\ndef intensive_task(duration: float):\n    \"\"\"Realiza una tarea intensiva en CPU durante un tiempo determinado.\"\"\"\n    end_time = time.time() + duration\n    while time.time() &lt; end_time:\n        _ = sum(x * x for x in range(10000))\n\n@app.get(\"/\")\nasync def root():\n    \"\"\"Simula una solicitud con retardo y carga en CPU.\"\"\"\n    delay = random.expovariate(SERVICE_RATE)  # Generar tiempo de espera exponencial\n    await asyncio.sleep(delay)  # Simular espera as\u00edncrona\n\n    intensive_task(delay)  # Cargar la CPU durante el mismo tiempo\n\n    return {\"message\": f\"Respuesta procesada con retraso y carga de {delay:.4f} segundos\"}\n</code></pre> <p>Este c\u00f3digo crea una API con FastAPI que simula un endpoint con retrasos variables y carga computacional, resultando \u00fatil para pruebas de estr\u00e9s.</p> <p>Endpoint / (Ruta principal):</p> <ul> <li>Encargado de crear los valores con promedio de 0.1 segundos (1/mu)</li> <li>Libera el servidor para atender otras peticiones</li> <li>por medio de la funcion stress_cpu(a) consume recursos durante el tiempo \"a\"</li> </ul>"},{"location":"coding/implementacion_py/#generador-de-trafico","title":"Generador de Trafico","text":"<pre><code>import asyncio\nimport aiohttp\nimport random\nimport time\nimport json\n\n# Configuraci\u00f3n\nURL = \"http://miapp.local:30240/\"  # Sustituye con la URL deseada\nLAMBDA = 4  # Tasa de arribo (inversa del tiempo medio entre solicitudes)\nSIMULATION_DURATION = 480  # Duraci\u00f3n de la simulaci\u00f3n en segundos\nOUTPUT_FILE = \"simulation_data.json\"  # Archivo de salida\n\n# Almacenamiento de datos\nrequests_data = []\n\nasync def send_request(request_number):\n    \"\"\"Env\u00eda una solicitud HTTP y registra la respuesta.\"\"\"\n    start_time = time.time()\n    send_timestamp = start_time\n    port = 10000 + request_number  # Cada solicitud usa un puerto distinto\n    conn = aiohttp.TCPConnector(local_addr=('0.0.0.0', port))\n\n    async with aiohttp.ClientSession(connector=conn) as session:\n        try:\n            async with session.get(URL) as response:\n                response_time = time.time() - start_time\n                requests_data.append({\n                    \"request_number\": request_number,\n                    \"response_time\": response_time,\n                    \"status_code\": response.status,\n                    \"send_timestamp\": send_timestamp\n                })\n        except Exception as e:\n            requests_data.append({\n                \"request_number\": request_number,\n                \"response_time\": time.time() - start_time,\n                \"status_code\": \"error\",\n                \"error\": str(e),\n                \"send_timestamp\": send_timestamp\n            })\n\nasync def request_generator():\n    \"\"\"Genera solicitudes basadas en una distribuci\u00f3n exponencial.\"\"\"\n    request_number = 0\n    simulation_start_time = time.time()\n\n    while time.time() - simulation_start_time &lt; SIMULATION_DURATION:\n        request_number += 1\n        asyncio.create_task(send_request(request_number))\n        await asyncio.sleep(random.expovariate(LAMBDA))\n\n    return time.time() - simulation_start_time\n\nasync def main():\n    \"\"\"Ejecuta la simulaci\u00f3n y guarda los resultados en un archivo JSON.\"\"\"\n    try:\n        simulation_time = await request_generator()\n    except asyncio.CancelledError:\n        simulation_time = None\n\n    simulation_output = {\n        \"total_simulation_time\": simulation_time or 0,\n        \"requests\": requests_data\n    }\n\n    with open(OUTPUT_FILE, \"w\") as f:\n        json.dump(simulation_output, f, indent=4)\n\n    return simulation_time\n\nif __name__ == \"__main__\":\n    try:\n        simulation_time = asyncio.run(main())\n        print(f\"Tiempo total de simulaci\u00f3n: {simulation_time:.2f} segundos.\")\n    except KeyboardInterrupt:\n        print(\"\\nSimulaci\u00f3n interrumpida manualmente.\")\n</code></pre>"},{"location":"coding/implementacion_py/#funcionamiento","title":"Funcionamiento","text":""},{"location":"coding/implementacion_py/#configuracion-inicial","title":"Configuracion inicial","text":"<p>El c\u00f3digo realiza una simulaci\u00f3n de tr\u00e1fico HTTP enviando solicitudes a una URL espec\u00edfica durante un tiempo determinado.</p> <p>Primero, se configuran los par\u00e1metros de la simulaci\u00f3n, como la URL de destino, la tasa de arribo (lambda), la duraci\u00f3n total de la simulaci\u00f3n y la lista requests_data, donde se almacenan los datos de cada solicitud.</p>"},{"location":"coding/implementacion_py/#funcion-para-el-envio-de-la-solicitud-http","title":"Funcion para el envio de la solicitud HTTP","text":"<p>La funci\u00f3n send_request(request_number) env\u00eda solicitudes HTTP de manera as\u00edncrona utilizando la biblioteca aiohttp. Para cada solicitud, se registra el tiempo de inicio y se asigna un puerto din\u00e1mico para evitar la congesti\u00f3n en un solo puerto. Luego, se establece una sesi\u00f3n HTTP con aiohttp.ClientSession() y se env\u00eda una petici\u00f3n GET a la URL definida. Se calcula el tiempo de respuesta y se almacena junto con el c\u00f3digo de estado HTTP en la lista requests_data. Si ocurre un error, se captura la excepci\u00f3n y se registra el tiempo transcurrido junto con el mensaje de error.</p>"},{"location":"coding/implementacion_py/#generador-de-solicitudes","title":"Generador de solicitudes","text":"<p>La funci\u00f3n request_generator() genera y lanza solicitudes de manera continua mientras dure la simulaci\u00f3n. Se inicializa un contador de solicitudes y se ejecuta un bucle que se mantiene activo hasta alcanzar la duraci\u00f3n total de la simulaci\u00f3n. Los tiempos entre solicitudes se generan de manera aleatoria siguiendo una distribuci\u00f3n exponencial con la tasa lambda. Cada solicitud se lanza de manera as\u00edncrona sin bloquear la ejecuci\u00f3n de otras solicitudes, y el generador espera el tiempo calculado antes de enviar la siguiente.</p>"},{"location":"coding/implementacion_py/#funcion-main","title":"Funcion main","text":"<p>La funci\u00f3n main() ejecuta la simulaci\u00f3n llamando a request_generator() y guarda los datos obtenidos en un archivo JSON. Este archivo contiene el tiempo total de simulaci\u00f3n y la informaci\u00f3n detallada de cada solicitud, incluyendo n\u00famero, tiempo de respuesta, c\u00f3digo de estado y posibles errores.</p> <p>En conjunto, el c\u00f3digo permite realizar pruebas de carga y evaluar el rendimiento de un servidor simulando tr\u00e1fico HTTP con intervalos de llegada aleatorios.</p>"},{"location":"coding/implementacion_py/#scripts-de-graficas","title":"Scripts de Graficas","text":""},{"location":"coding/implementacion_py/#tiempo-de-respuesta-del-sistema-en-funcion-de-la-carga-y-el-uso-de-cpu","title":"Tiempo de respuesta del sistema en funcion de la carga y el uso de CPU:","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\n# ESCENARIO - 1 POD\n#TEORICO\nmu10 = 10  # Tasa de servicio fija\nrho_values_t10 = np.linspace(0, 0.99, 100)\nfactor_delay10 = 1 / (mu10 * (1 - rho_values_t10))\n\n#PR\u00c1CTICO\n# Valores experimentales de \u03bb (tasa de llegada) y \u03bc (tasa de servicio)\nlambda_values1 = [1.04, 3.08, 6.01, 8.66, 9.7]   # Valores de lambda obtenidos\nmu_values1 = [9.45, 9.56, 9.55, 9.84, 9.8]    # Valores de mu calculados \n# Calcular \u03c1 (carga del sistema) y factor de delay W\nrho_values_p1 = [lam / mu for lam, mu in zip(lambda_values1, mu_values1)]\nfactor_delay1 = [1 / (mu - lam) if mu &gt; lam else np.inf  # Evita divisi\u00f3n por 0 o inestabilidad\nfor lam, mu in zip(lambda_values1, mu_values1)]\n\n# Graficar la curva pr\u00e1ctica \u03c1 vs W\nplt.figure(figsize=(8, 6))\nplt.xlim(0,1)  # Ajusta el eje X entre 0 y 1\nplt.ylim(0,3)  # Ajusta el eje Y entre 0 y 10\nplt.plot(rho_values_t10, factor_delay10, color=\"b\", label=f\"pods=1\", linestyle=\"--\")\nplt.plot(rho_values_p1, factor_delay1, marker='o', linestyle='none', color='r', label=\"Puntos Pr\u00e1cticos\")\nplt.axvline(x=1, color='r', linestyle='--', label=\"L\u00edmite \u03c1=1\")\nplt.xlabel(\"Carga del Sistema \u03c1\")\nplt.ylabel(\"Factor de Delay (W)\")\nplt.title(\"Curva Pr\u00e1ctica del Factor Delay vs Carga del Sistema\")\nplt.grid(True)\nplt.legend()\nplt.show()\nplt.savefig(\"Factor_Delay_rho.png\")\n\n# Gr\u00e1fico 2: Uso de CPU vs Tasa de Arribo\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_values1, rho_values_p1, marker='s', color='m', linestyle='-', label=\"\u03bb vs \u03c1\")\nplt.xlabel(\"Tasa de Arribo (\u03bb)\")\nplt.ylabel(\"Uso del CPU (%\")\nplt.title(\"Uso de CPU (%) vs Tasa de Arribo\")\nplt.grid(True)\nplt.legend()\nplt.savefig(\"Cpu_vs_Lambda.png\")\nplt.show()\n</code></pre>"},{"location":"coding/implementacion_py/#numero-de-perdidas-en-funcion-de-la-tasa-de-arrivo","title":"N\u00famero de perdidas en funcion de la tasa de arrivo","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\ndef loss_rate(lam, mu, servers=1):\n    \"\"\"\n    Calcula la tasa de p\u00e9rdida de solicitudes en un sistema M/M/k/k (Erlang-B).\n\n    Par\u00e1metros:\n    - lam: tasa de llegada (\u03bb)\n    - mu: tasa de servicio (\u03bc)\n    - servers: n\u00famero de servidores disponibles (k)\n\n    Retorna:\n    - Probabilidad de que una solicitud sea rechazada (P_loss)\n    \"\"\"\n    rho = lam / (servers * mu)  # Carga del sistema\n    if rho &gt;= 1:\n        return 1  # Si la carga es &gt;=1, todas las solicitudes nuevas ser\u00e1n rechazadas\n\n    # F\u00f3rmula de Erlang-B para p\u00e9rdida de solicitudes\n    erlang_b = ( (lam/mu)**servers / math.factorial(servers) ) / sum((lam/mu)**n / math.factorial(n) for n in range(servers + 1))\n\n    return erlang_b\n\n# Definir valores de \u03bb y \u03bc para la simulaci\u00f3n\nlambda_values = [1.04, 3.08, 6.01, 8.66, 9.7]\nmu_values = [9.45, 9.56, 9.55, 9.84, 9.8]\nservers = 1  # N\u00famero de pods o servidores\n\n# Calcular tasa de p\u00e9rdida\nloss_rates = [loss_rate(lam, mu, servers) for lam, mu in zip(lambda_values, mu_values)]\n\n# Graficar la tasa de p\u00e9rdida en funci\u00f3n de la tasa de arribo (\u03bb)\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_values, loss_rates, marker='s', color='r', linestyle='-', label=\"Tasa de P\u00e9rdida vs \u03bb\")\nplt.xlabel(\"Tasa de Arribo (\u03bb)\")\nplt.ylabel(\"Tasa de P\u00e9rdida de Solicitudes\")\nplt.title(\"Tasa de P\u00e9rdida vs Tasa de Arribo\")\nplt.grid(True)\nplt.legend()\nplt.savefig(\"Loss_Rate_vs_Lambda.png\")\nplt.show()\n</code></pre>"},{"location":"coding/implementacion_py/#numero-de-tareas-en-funcion-de-la-tasa-de-arrivo","title":"Numero de tareas en funcion de la tasa de arrivo","text":"<pre><code>import numpy as np\nimport math\nimport matplotlib.pyplot as plt\n\n# Par\u00e1metros del sistema\nmu = 10  # Tasa de servicio de cada pod\nk = 1  # N\u00famero de pods (servidores)\n\n# Rango de tasas de arribo (lambda)\nlambda_values = np.linspace(1, 49, 100)  # De 1 a 49 solicitudes/segundo\n\ndef p0(lambda_val, mu, k):\n    \"\"\" Calcula la probabilidad de que no haya solicitudes en el sistema \"\"\"\n    rho = lambda_val / (k * mu)\n    sumatoria = sum((lambda_val / mu) ** n / math.factorial(n) for n in range(k + 1))\n    sumatoria += ((lambda_val / mu) ** k) / (math.factorial(k) * (1 - rho))\n    return 1 / sumatoria\n\ndef p_q(lambda_val, mu, k):\n    \"\"\" Calcula la probabilidad de que una solicitud tenga que esperar en cola \"\"\"\n    rho = lambda_val / (k * mu)\n    if rho &gt;= 1:  # Evitar inestabilidad matem\u00e1tica\n        return 1\n    p0_val = p0(lambda_val, mu, k)\n    numerator = (p0_val * (lambda_val / mu) ** k) / (math.factorial(k) * (1 - rho))\n    denominator = 1 + ((1 - rho) / rho) * (1 - numerator)\n    return numerator / denominator\n\ndef l_q(lambda_val, mu, k):\n    \"\"\" Calcula el n\u00famero esperado de tareas en la cola \"\"\"\n    rho = lambda_val / (k * mu)\n    if rho &gt;= 1:  # Evitar inestabilidad\n        return np.inf\n    return (p_q(lambda_val, mu, k) * (lambda_val / mu)) / (1 - rho)\n\ndef l_s(lambda_val, mu, k):\n    \"\"\" Calcula el n\u00famero esperado de tareas en servicio \"\"\"\n    return lambda_val / mu\n\ndef l_total(lambda_val, mu, k):\n    \"\"\" Calcula el n\u00famero total esperado de tareas en el sistema \"\"\"\n    return l_q(lambda_val, mu, k) + l_s(lambda_val, mu, k)\n\n# Calcular L (n\u00famero de tareas en el sistema) para cada lambda\nl_values = [l_total(lam, mu, k) for lam in lambda_values]\n\n# Graficar n\u00famero de tareas en el sistema vs tasa de arribo\nplt.figure(figsize=(8, 6))\nplt.plot(lambda_values, l_values, marker='o', linestyle='-', color='b', label=\"N\u00famero de Tareas (L)\")\n\nplt.xlabel(\"Tasa de Arribo \u03bb (Solicitudes/seg)\")\nplt.ylabel(\"N\u00famero de Tareas en el Sistema (L)\")\nplt.title(f\"N\u00famero de Tareas vs Tasa de Arribo (k={k} pods, \u03bc={mu})\")\nplt.legend()\nplt.grid(True)\n\n# Guardar y mostrar el gr\u00e1fico\nplt.savefig(\"Numero_Tareas_vs_Lambda.png\")\nplt.show()\n</code></pre>"},{"location":"coding/implementacion_py/#tiempo-de-respuesta-del-sistema-en-funcion-de-la-cantidad-de-pods","title":"Tiempo de respuesta del sistema en funcion de la cantidad de PODs","text":"<pre><code>#RESPUESTA DEL SISTEMA EN FUNCION DE la cantidad de servidores (PODS)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definir los valores de la tasa de servicio (\u03bc) y la tasa de llegada (\u03bb)\nmu = 10  # Tasa de servicio de cada pod (solicitudes por segundo)\nlambda_fixed = 15  # Tasa de llegada total de solicitudes (solicitudes por segundo)\n\n# Rango de pods (cantidad de servidores en paralelo)\nk_values = np.arange(1, 11, 1)  # De 1 a 10 pods\n\n# Calcular la carga del sistema (\u03c1) para cada k\nrho_values = lambda_fixed / (k_values * mu)\n\n# Calcular el tiempo de respuesta te\u00f3rico W = 1 / (mu * (1 - rho)), evitando divisi\u00f3n por 0\nW_theoretical = np.where(rho_values &lt; 1, 1 / (mu * (1 - rho_values)), np.inf)\n\n# Simulaci\u00f3n de valores pr\u00e1cticos con peque\u00f1as variaciones aleatorias\nW_practical = W_theoretical * (1 + np.random.uniform(-0.1, 0.1, size=len(k_values)))\n\n# Graficar los resultados\nplt.figure(figsize=(8, 6))\nplt.plot(k_values, W_theoretical, label=\"Tiempo de Respuesta Te\u00f3rico\", color=\"b\", linestyle='--')\nplt.scatter(k_values, W_practical, label=\"Datos Pr\u00e1cticos\", color=\"r\", marker='o')\n\nplt.xlabel(\"Cantidad de Pods (Servidores en el Cl\u00faster)\")\nplt.ylabel(\"Tiempo de Respuesta Promedio (W)\")\nplt.title(\"Tiempo de Respuesta del Sistema vs. Cantidad de Pods\")\nplt.legend()\nplt.grid(True)\n\n# Guardar y mostrar\nplt.savefig(\"Tiempo_Respuesta_vs_Pods.png\")\nplt.show()\n</code></pre>"},{"location":"coding/implementacion_yml/","title":"yaml","text":""},{"location":"coding/implementacion_yml/#deployment","title":"Deployment","text":"<p><pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: miapp-app\nspec:\n  replicas: 2 # N\u00famero de r\u00e9plicas del pod\n  selector:\n    matchLabels:\n      app: miapp-app\n  template:\n    metadata:\n      labels:\n        app: miapp-app\n    spec:\n      containers:\n        - name: miapp-app\n          image: ramirotizzian1/miapp:latest\n          ports:\n            - containerPort: 8000  # Exponemos el puerto 8000 de la aplicaci\u00f3n\n          resources:\n            requests:\n              cpu: 500m #CPU asignada a cada pod, puede ser menor al l\u00edmite m\u00e1ximo\n            limits:\n              cpu: 1000m #CPU m\u00e1xima permitida a cada pod\n</code></pre> Explicacion basica de funcionamiento</p>"},{"location":"coding/implementacion_yml/#service","title":"Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: miapp-service\n  annotations:\n    service.kubernetes.io/ipvs-scheduler: \"lc\"\nspec:\n  selector:\n    app: miapp-app\n  ports:\n    - protocol: TCP\n      port: 80  # El puerto que se expondr\u00e1 internamente\n      targetPort: 8000  # El puerto de la aplicaci\u00f3n FastAPI\n  type: ClusterIP\n</code></pre>"},{"location":"coding/implementacion_yml/#ingress","title":"Ingress","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: miapp-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/load-balance: \"round_robin\"\nspec:\n  rules:\n    - host: miapp.local  # Puedes usar cualquier nombre de dominio, en este caso es miapp.local\n      http:\n        paths:\n        - path: /\n          pathType: Prefix\n          backend:\n            service:\n              name: miapp-service\n              port:\n                number: 80\n</code></pre>"},{"location":"coding/implementacion_yml/#horizontal-pod-autoescaler","title":"Horizontal POD Autoescaler","text":"<pre><code>apiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: miapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: miapp-app\n  minReplicas: 1\n  maxReplicas: 6\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 25  # Escala si el uso promedio de CPU excede el 25%\n</code></pre>"}]}