
## Consideraciones

Para realizar las pruebas de carga se tuvieron en cuenta las siguientes consideraciones:

1. La tasa de servicio del servidor es =10 como parametro de la distribucion exponencial. Definido en el archivo [server.py](./coding/implementacion_py.md/#servidor-de-respuestas-a-peticiones-http).
2. La tasa de arrivo  se varia aumentando la carga en cada una de las pruebas.
3. La utilizaci贸n del sistema = / ser谩 el valor que se desea obtener en
las pruebas.
4. El tiempo de respuesta por solicitud T=1/(-), el cual se va a utilizar para comprar los valores teoricos y los obtenidos en la practica, para mas de un pod se calcula como T=1/(*k-).



### Tabla I: Resultados de las mediciones
![Tabla I](img/figura6.png)


En la Tabla 1 se presentan los distintos escenarios dise帽ados para analizar el comportamiento del sistema. El ensayo consisti贸 en realizar 28 pruebas distintas, en las cuales se modificaron diferentes par谩metros con el objetivo de evaluar los resultados obtenidos en cada caso. Durante las pruebas, se vari贸 la cantidad de pods en el cl煤ster y, para cada configuraci贸n, se ajustaron los valores te贸ricos de 位 (tasa de llegada de solicitudes) en funci贸n de la carga del sistema  que se deseaba analizar.

En la pr谩ctica, se observ贸 el comportamiento real de los pods y se calcul贸 el promedio de utilizaci贸n de milicores en el cl煤ster. Con este valor, se obtuvo el  pr谩ctico, lo que permiti贸 determinar el 渭 pr谩ctico bas谩ndose en la ecuaci贸n del sistema para un modelo M/M/C, donde el n煤mero de servidores C corresponde a la cantidad de pods configurados en el cl煤ster.



## Ecuaciones utilizadas para modelar el sistema

![Modelo](img/figura7.png)

Donde *k* son la cantidad de PODS.
***

## **Tiempos de Respuesta del sistema en funcion de la tasa de arribo:**

Para valores bajo de carga del sistema , el tiempo de respuesta es peque帽o y estable, pero cuando se acerca a 1 o lo que es lo mismo cuando  位 se acerca demasiado a 渭, el tiempo de respuesta crece r谩pidamente y el pod ya no puede manejar la carga de trabajo de manera efectiva lo que indica congestion y demoras altas.

![Modelo](img/trespuesta_vs_rho.png)



## **Numero de tareas en el sistema en funcion de la tasa de arribo:** 

Para valores bajos de 位, el numero esperado de tareas en el sistema crece de manera controlado, pero a medida que la tasa de arrivos se acerca a la capacidad del sistema (k渭), L se dispara, indicando sobrecarga y congestion. Cuando 位 supera la capacidad del sistema, L tiende a infinito, y el sistema es inestable

![Modelo](img/Numero_Tareas_vs_Lambda.png)



## **Consumo de CPU en el cluster en funcion de la tasa de arribo:**

El consumo de CPU en el cluster para los valores de lambda tomados crece en porcentaje a medida que se incrementa la tasa de arrivos, la utilidad en el sistema aumenta por lo tanto es mayor el consumo de recursos de hardware.

![Modelo](img/Cpu_vs_Lambda.png)


## Uso de CPU en la maquina anfitriona con el cluster corriendo en reposo y realizando una prueba de carga:

![Modelo](img/cluster_en_reposo.png)
![Modelo](img/uso_cpu_6podsl50.png)




## **Tasa de perdida en funcion de la tasa de arribo:**

Al aumentar la tasa de arribo 位, el sistema tiende a saturarse, lo que provoca un aumento en el tiempo de respuesta, una mayor tasa de p茅rdida de solicitudes y una alta utilizaci贸n de recursos.

![Modelo](img/Loss_Rate_vs_Lambda.png)

&nbsp;

## *Respuesta del sistema en funcion de la cantidad de servidores en ejecucion para un lambda=15 1/s y mu=10 1/s*

![Modelo](img/Tiempo_Respuesta_vs_Pods.png)


[*Scripts para graficar*](./coding/implementacion_py.md/#scripts-de-graficas).

&nbsp;


## **Tiempo de respuesta promedio en funcion del factor de utilidad para los valores medidos variando la cantidad de pods:**

![Modelo](img/curvas_respuesta_carga.png)

Se observa que a medida que se incrementan la cantidad de servidores disponibles, la curva de tiempo de respuesta en el sistema es menor.

## Uso del Horizontal Pod Autoescaler

### Caso 1: 1 Pod, 位=30
![Modelo](img/t_respuestal30_1pod (1).png)

Configurando el HPA para que escale hasta un maximo de 6 pods cuando el uso promedio del CPU exceda el 25%, para este caso con una tasa de arribo igual a 30 1/s y para un pod en ejecucion el sistema se estabiliza aproximadamente a las 7000 solicitudes alcanzando el escalado maximo de 6 replicas.

### Caso 2: 2 Pod, 位=30

![Modelo](img/t_respuesta_l302pods.png)

### Caso 3: 2 Pod, 位=40

![Modelo](img/t_respuesta_l402pods.png)

[*Horizontal Pod Autoescaler*](./coding/implementacion_yml.md/#horizontal-pod-autoescaler)
